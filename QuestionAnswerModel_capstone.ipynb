{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a02c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fd0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert/distilbert-base-uncased-distilled-squad'\n",
    "# you may change the model for better performance or use chat-gpt api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e137f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name,output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24971e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('main_txt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5b71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b911a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df.sample(n=10000, random_state=5)\n",
    "# the dataset contains nearly 27000 queries and responses\n",
    "# due to computation issue , i have taken 2000 but for better performance take more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0f983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[{}]', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('oorder', 'order')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0974959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled['query'] = df_sampled['query'].apply(preprocess_text)\n",
    "df_sampled['response'] = df_sampled['response'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42dd2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    last_hidden_states = outputs.hidden_states[-1]  # Get the last hidden states\n",
    "    return last_hidden_states[:, 0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3171acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "def process_in_batches(texts, batch_size=32, save_path='embeddings'):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    embeddings = []\n",
    "    checkpoint_file = os.path.join(save_path, 'checkpoint.npy')\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        start_batch = np.load(checkpoint_file)\n",
    "        print(f\"Resuming from batch {start_batch}\")\n",
    "    else:\n",
    "        start_batch = 0\n",
    "    \n",
    "    for i in range(start_batch, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = get_embeddings(batch_texts)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "        \n",
    "        np.save(os.path.join(save_path, f'embeddings_batch_{i//batch_size}.npy'), batch_embeddings)\n",
    "        np.save(checkpoint_file, np.array([i + batch_size]))\n",
    "    \n",
    "    if embeddings:\n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "    else:\n",
    "        all_embeddings = []\n",
    "        for file in sorted(os.listdir(save_path)):\n",
    "            if file.startswith('embeddings_batch_') and file.endswith('.npy'):\n",
    "                all_embeddings.append(np.load(os.path.join(save_path, file)))\n",
    "        return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89eb04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_in_batches_res(texts, batch_size=32, save_path='embeddings_res'):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    embeddings = []\n",
    "    checkpoint_file = os.path.join(save_path, 'checkpoint_res.npy')\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        start_batch = np.load(checkpoint_file)\n",
    "        print(f\"Resuming from batch {start_batch}\")\n",
    "    else:\n",
    "        start_batch = 0\n",
    "    \n",
    "    for i in range(start_batch, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = get_embeddings(batch_texts)\n",
    "        embeddings.append(batch_embeddings)  \n",
    "        \n",
    "        np.save(os.path.join(save_path, f'embeddings_batch_{i//batch_size}.npy'), batch_embeddings)\n",
    "        np.save(checkpoint_file, np.array([i + batch_size]))\n",
    "    \n",
    "    if embeddings:\n",
    "        return np.concatenate(embeddings, axis=0)  \n",
    "    else:\n",
    "        all_embeddings = []\n",
    "        for file in sorted(os.listdir(save_path)):\n",
    "            if file.startswith('embeddings_batch_') and file.endswith('.npy'):\n",
    "                all_embeddings.append(np.load(os.path.join(save_path, file)))\n",
    "        return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00f52e",
   "metadata": {},
   "source": [
    "**The embeddings files in the repo are for sampled 2000 datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c22fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = process_in_batches(df_sampled['query'].tolist(), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eaab181",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_embeddings = process_in_batches_res(df_sampled['response'].tolist(), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88f42b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "queries_embeddings_np = query_embeddings\n",
    "responses_embeddings_np = response_embeddings\n",
    "\n",
    "dimension = queries_embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "index.add(queries_embeddings_np)\n",
    "\n",
    "faiss.write_index(index, \"queries.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94bb6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "response_index.add(responses_embeddings_np)\n",
    "\n",
    "faiss.write_index(response_index, \"responses.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9976c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_responses(query_embedding, response_index, k=10):\n",
    "    D, I = response_index.search(query_embedding, k)\n",
    "    return I \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1aa80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(query, model, tokenizer, response_index, responses, k=10):\n",
    "    query_embedding = get_embeddings([query])[0].reshape(1, -1)\n",
    "    \n",
    "    closest_responses_indices = retrieve_responses(query_embedding, response_index, k)\n",
    "    \n",
    "    retrieved_contexts = [responses[i] for i in closest_responses_indices[0]]\n",
    "    \n",
    "    combined_context = \" \".join(retrieved_contexts)\n",
    "    \n",
    "    inputs = tokenizer(query, combined_context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    print(\"Inputs to the model:\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    \n",
    "    print(\"Start logits:\", outputs.start_logits)\n",
    "    print(\"End logits:\", outputs.end_logits)\n",
    "    \n",
    "    return answer, closest_responses_indices\n",
    "\n",
    "question = \"How to cancel order?\"\n",
    "responses = df_sampled['response'].tolist()  \n",
    "answer, retrieved_responses_indices = predict_answer(question, model, tokenizer, response_index, responses)\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")\n",
    "print(f\"Retrieved Responses Indices: {retrieved_responses_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaba203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
